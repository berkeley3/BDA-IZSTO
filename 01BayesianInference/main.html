<!DOCTYPE html>
<html>
<head>
  <title>Bayesian Data Analysis for Medical Data</title>

  <meta charset="utf-8">
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="generator" content="pandoc" />




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">

  <base target="_blank">

  <script type="text/javascript">
    var SLIDE_CONFIG = {
      // Slide settings
      settings: {
                title: 'Bayesian Data Analysis for Medical Data',
                        subtitle: 'Bayesian Inference',
                useBuilds: true,
        usePrettify: true,
        enableSlideAreas: true,
        enableTouch: true,
                        favIcon: 'main_files/logo.jpg',
              },

      // Author information
      presenters: [
            {
        name:  'Paola Berchialla' ,
        company: '',
        gplus: '',
        twitter: '',
        www: '',
        github: ''
      },
            ]
    };
  </script>

  <link href="main_files/ioslides-13.5.1/fonts/fonts.css" rel="stylesheet" />
  <link href="main_files/ioslides-13.5.1/theme/css/default.css" rel="stylesheet" />
  <link href="main_files/ioslides-13.5.1/theme/css/phone.css" rel="stylesheet" />
  <script src="main_files/ioslides-13.5.1/js/modernizr.custom.45394.js"></script>
  <script src="main_files/ioslides-13.5.1/js/prettify/prettify.js"></script>
  <script src="main_files/ioslides-13.5.1/js/prettify/lang-r.js"></script>
  <script src="main_files/ioslides-13.5.1/js/prettify/lang-yaml.js"></script>
  <script src="main_files/ioslides-13.5.1/js/hammer.js"></script>
  <script src="main_files/ioslides-13.5.1/js/slide-controller.js"></script>
  <script src="main_files/ioslides-13.5.1/js/slide-deck.js"></script>

  <style type="text/css">

    b, strong {
      font-weight: bold;
    }

    em {
      font-style: italic;
    }

    slides > slide {
      -webkit-transition: all 0.4s ease-in-out;
      -moz-transition: all 0.4s ease-in-out;
      -o-transition: all 0.4s ease-in-out;
      transition: all 0.4s ease-in-out;
    }

    .auto-fadein {
      -webkit-transition: opacity 0.6s ease-in;
      -webkit-transition-delay: 0.4s;
      -moz-transition: opacity 0.6s ease-in 0.4s;
      -o-transition: opacity 0.6s ease-in 0.4s;
      transition: opacity 0.6s ease-in 0.4s;
      opacity: 0;
    }

    slides > slide:not(.nobackground):before {
      font-size: 12pt;
      content: "";
      position: absolute;
      bottom: 20px;
      left: 60px;
      background: url(main_files/logo.jpg) no-repeat 0 50%;
      -webkit-background-size: 30px 30px;
      -moz-background-size: 30px 30px;
      -o-background-size: 30px 30px;
      background-size: 30px 30px;
      padding-left: 40px;
      height: 30px;
      line-height: 1.9;
    }
  </style>

  <link rel="stylesheet" href="assets\css\ioslides.css" type="text/css" />

</head>

<body style="opacity: 0">

<slides class="layout-widescreen">

  <slide class="title-slide segue nobackground">
        <aside class="gdbar"><img src="main_files/logo.jpg"></aside>
        <!-- The content of this hgroup is replaced programmatically through the slide_config.json. -->
    <hgroup class="auto-fadein">
      <h1 data-config-title><!-- populated from slide_config.json --></h1>
      <h2 data-config-subtitle><!-- populated from slide_config.json --></h2>
      <p data-config-presenter><!-- populated from slide_config.json --></p>
          </hgroup>
  </slide>

<slide class=''><hgroup><h2>Repeated i.i.d. trials</h2></hgroup><article  id="repeated-i.i.d.-trials">

<ul>
<li>Suppose to generate a outcome from among several potential outcomes</li>
<li>Suppose outcome chances are the same each time

<ul>
<li>outcomes are independent and identically distributed (i.i.d)</li>
</ul></li>
<li>Suppose the outcome is binary (e.g. 0,1; dead/alive,&#8230;)

<ul>
<li>20% chance of outcome 1</li>
<li>80% chance of outcome 0</li>
</ul></li>
<li>Consider different numbers of trials

<ul>
<li>how will proportion of successes in sample differ?</li>
</ul></li>
</ul>

</article></slide><slide class=''><hgroup><h2>Simulating i.i.d. Binary Trials</h2></hgroup><article  id="simulating-i.i.d.-binary-trials">

<pre class = 'prettyprint lang-r'>rbinom(10, N, 0.2)/N</pre>

<p>10 trials: (0% to 30% success rate)</p>

<p>3 0 2 1 3 2 1 1 3 2</p>

<p>100 trials: (17% to 24% success rate)</p>

<p>22 17 18 21 19 20 18 24 22 18</p>

<p>100 trials: (19.2% to 22.9% success rate)</p>

<p>229 206 201 196 198 192 198 203 219 199</p>

<p>100 trials: (19.6% to 20.7% success rate)</p>

<p>1958 2071 1992 1999 1996 1984 1983 2034 2041 2007</p>

</article></slide><slide class=''><hgroup><h2>Simple point estimation</h2></hgroup><article  id="simple-point-estimation">

<ul>
<li><p>Estimate chance of success \(\theta\) by proportion of successes \[
\theta^* = \frac{\text{successes}}{\text{trials}}
\]</p></li>
<li><p>Simulation shows accuraty depends on amount of data</p></li>
</ul>

</article></slide><slide class=''><hgroup><h2>Example Interval Calculation</h2></hgroup><article  id="example-interval-calculation">

<ul>
<li>P% <em>confidence interval:</em> interval in which P% of the estimates are expected to fall</li>
<li>Simulation computes intervals to any accuracy</li>
<li>How to do: simulate, sort, inspect the central empirical interval</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Example Interval Calculation</h2></hgroup><article  id="example-interval-calculation-1">

<ul>
<li>To calcualte 95% confidence interval of estimate based on 10000 samples</li>
</ul>

<pre class = 'prettyprint lang-r'>sims &lt;- rbinom(10000, 1000, 0.2)/1000
quantile(sims, probs = c(0.025, 0.975))</pre>

<p>The 95% confidence interval is: ( 0.176 , 0.226 )</p>

</article></slide><slide class=''><hgroup><h2>Estimator Bias</h2></hgroup><article  id="estimator-bias">

<ul>
<li><p><em>Bias:</em> expected difference of estimate from true value</p></li>
<li><p>continuous previous examples</p></li>
</ul>

<pre class = 'prettyprint lang-r'>sims &lt;- rbinom(10000, 1000, 0.2)/1000
quantile(sims, probs = 0.5))</pre>

<pre class = 'prettyprint lang-r'>sims &lt;- rbinom(10000, 1000, 0.2)/1000
quantile(sims, probs = 0.5)</pre>

<p>50% 0.2</p>

<ul>
<li>Central value of 0.2 shows this estiamator is <strong>unbiased</strong></li>
</ul>

</article></slide><slide class=''><hgroup><h2>Simple Point Estimation</h2></hgroup><article  id="simple-point-estimation-1">

<ul>
<li><em>Central Limit Theorem:</em> <strong>expected</strong> error in \(\theta^{*}\) goes down as \[
\frac{1}{\sqrt{N}}
\]</li>
<li>a decimal place of accuracy requires \(100\times\) more samples</li>
<li>The width of confidence intervals shrinks at the same rate</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Quiz!</h2></hgroup><article  id="quiz">

<p><img src="main_files/figure-html/quiz-1.png" width="720" style="display: block; margin: auto;" /></p>

</article></slide><slide class=''><hgroup><h2>Quiz!</h2></hgroup><article  id="quiz-1">

<p><img src="main_files/figure-html/quiz2-1.png" width="720" style="display: block; margin: auto;" /></p>

</article></slide><slide class=''><hgroup><h2>Quiz Answer</h2></hgroup><article  id="quiz-answer">

<ul>
<li>Mix simulation of repeated i.i.d trials with 20% of success and sort</li>
</ul>

<pre >## 0/10 0/10 0/10 0/10 1/10 
##  
##  1/10 14/100 15/100 16/100 186/1000 
##  
##  19/100 190/1000 192/1000 196/1000 2/10 
##  
##  2/10 20/100 201/1000 202/1000 204/1000 
##  
##  206/1000 21/100 21/100 21/100 210/1000 
##  
##  224/1000 24/100 25/100 3/10 3/10</pre>

<ul class = 'build'>
<li>More variation in observed rates with smaller sample sizes</li>
</ul>

<ul class = 'build'>
<li><em>Answer:</em> High cancer and low cancer counties are small populations</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Observations, conterfactuals and random variables</h2></hgroup><article  id="observations-conterfactuals-and-random-variables">

<ul>
<li>Assume we observe data \(\mathbf y = y_1,\ldots, y_N\)</li>
<li>Statistical modelling assumes even though \(\mathbf y\) is observed, the values could have been different</li>
<li>John Stuart Mill first characterized this <em>conterfactual</em> nature of statistical modelling

<ul>
<li>A System of Logic, Rationative and Inductive (1843)</li>
</ul></li>
<li>In modern (Kolmogorov) language, \(\mathbf y\) is a <em>random variable</em></li>
</ul>

</article></slide><slide class=''><hgroup><h2>Likelihood functions</h2></hgroup><article  id="likelihood-functions">

<ul>
<li>A <em>likelihood function</em> is a probability function

<ul>
<li>density, mass or mixed \[
p(y\vert\theta , x)
\] where \(\theta\) is a vector of <strong>parameters</strong></li>
</ul></li>
<li>\(x\) is some fixed data

<ul>
<li>regression predictors or <em>features</em></li>
</ul></li>
<li>A <strong>likelihood function</strong> is considered as a function \(L(\theta)\) of \(\theta\) for fixed \(x\) and \(y\)</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Maximum Likelihood Estimation</h2></hgroup><article  id="maximum-likelihood-estimation">

<ul>
<li><p>The statistical inference problem is to estimate parameters \(\theta\) given observations \(y\)</p></li>
<li>Maximum Likelihood Estimation (MLE) chooses the estimate \(\theta^*\) that meximizes the likelihood function \[
\theta^* = \text{arg max}_\theta L(\theta) = \text{arg max}_\theta p(y\vert\theta, x)
\]</li>
<li><p>this function of \(L\) and \(y\) (and \(x\)) is called <strong>estimator</strong></p></li>
</ul>

</article></slide><slide class=''><hgroup><h2>Example of MLE</h2></hgroup><article  id="example-of-mle" class="smaller">

<ul>
<li><p>The frequency-based estimate \[
\theta^* = \frac{1}{N}\sum_{i=1}^N y_i
\] is the observed rate of <em>success</em> (outcome 1) observations</p></li>
<li><p>The MLE for the model \[
p(y\vert\theta) = \prod_{i=1}^N p(y_i\vert\theta) = \prod_{i=1}^N \text{Bernoulli}(y_i\vert\theta)
\] where for \(u\in\{0,1\}\) \[
\text{Bernoulli}(y_i\vert\theta) =\left\{ \begin{array}{ll}
\theta &amp; \text{if } u=1\\
1-\theta &amp; \text{if } u=0\\
\end{array}
\right.
\]</p></li>
</ul>

</article></slide><slide class=''><hgroup><h2>Example of MLE</h2></hgroup><article  id="example-of-mle-1" class="smaller">

<ul>
<li>The frequency-based estimate is the MLE

<ul>
<li>derivative is zero (indicating min or max) \[
L^{&#39;}(\theta^*) =0
\]</li>
<li>second derivative is negative (indicating max) \[
L^{&#39;&#39;}(\theta^*) &lt; 0
\]</li>
</ul></li>
</ul>

</article></slide><slide class=''><hgroup><h2>Example of MLE</h2></hgroup><article  id="example-of-mle-2" class="smaller">

<ul>
<li>First modelling <strong>assumption</strong> is that data are i.i.d.</li>
</ul>

<p>\[
p(y\vert\theta) = \prod_{i=1}^N p(y_i\vert\theta)
\]</p>

<ul>
<li>Second modelling <strong>assumption</strong> is form of likelihood \[
p(y_i\vert\theta) = \text{Bernoulli}(y_i\vert\theta)
\]</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Beware: MLEs can be dangerous</h2></hgroup><article  id="beware-mles-can-be-dangerous">

<ul>
<li>Recall cancer cluster example</li>
<li>Accuracy is low with smalls counts</li>
<li>Hierarchical models are what we need</li>
</ul>

</article></slide><slide class='segue dark nobackground level1'><hgroup class = 'auto-fadein'><h2>Bayesian Inference</h2></hgroup><article  id="bayesian-inference">

</article></slide><slide class=''><hgroup><h2>Bayesian Data Analysis</h2></hgroup><article  id="bayesian-data-analysis">

<ul>
<li><p>By Bayesian data analysis, we mean practical methods for making inferences from data using probability models for quantities we observe and about which we wish to learn</p></li>
<li><p>The essential characteristic of Bayesian methods is their <strong>explicit use of probability for quantifying uncertainty</strong> in inferences based on statistical analysis</p></li>
<li><p>from Gelman et al. <strong>Bayesian Data Analysis</strong> 3rd edition 2013</p></li>
</ul>

</article></slide><slide class=''><hgroup><h2>Bayesian Mechanics</h2></hgroup><article  id="bayesian-mechanics">

<!-- THIS IS A COMMENTED TEXT EVEN FOR MARKDOWN :-) -->

<ol>
<li>Set up full probability model

<ul>
<li>for all observable &amp; unobservable quantities</li>
<li>consistent with problem knowledge &amp; data collection</li>
</ul></li>
<li>Condition on observed data

<ul>
<li>calculate posterior probability of unobserved quantities conditional on observed quantities</li>
</ul></li>
<li>Evaluate

<ul>
<li>model fit</li>
<li>implications of posterior</li>
</ul></li>
</ol>

</article></slide><slide class=''><hgroup><h2>Notation</h2></hgroup><article  id="notation">

<ul>
<li>Basic Quantities

<ul>
<li>\(y\): observed data</li>
<li>\(\theta\): parameters (and other unobserved quantities)</li>
<li>\(x\): constants, predictors for conditional models</li>
</ul></li>
<li>Basic Predictive Quantities

<ul>
<li>\(\tilde y\): unknown, potentially observable quantities</li>
<li>\(\tilde x\): predictors for unknown quantities</li>
</ul></li>
</ul>

</article></slide><slide class=''><hgroup><h2>Naming Conventions</h2></hgroup><article  id="naming-conventions">

<ul>
<li><p><strong>Joint:</strong> \(p(y,\theta)\)</p></li>
<li><strong>Sampling/Likelihood:</strong> \(p(y\vert \theta)\)

<ul>
<li>Sampling is function of \(y\) with \(\theta\) fixed (probability function)</li>
<li>Likelihood is function of \(\theta\) with \(y\) fixed (<em>not</em> probability function)</li>
</ul></li>
<li><strong>Prior:</strong> \(p(\theta)\)</li>
<li><strong>Posterior:</strong> \(p(\theta\vert y)\)</li>
<li><strong>Data Marginal (Evidence):</strong> \(p(y)\)</li>
<li><p><strong>Posterior Predictive:</strong> \(p(\tilde y\vert y)\)</p></li>
</ul>

</article></slide><slide class=''><hgroup><h2>Bayes&#39;s Rule for Posterior</h2></hgroup><article  id="bayess-rule-for-posterior">



<ul>
<li><em>Inversion:</em> Final result depends only on sampling distribution (likelihood) \(p(y\vert\theta)\) and prior \(p(\theta)\)</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Bayes&#39;s Rule up to Proportion</h2></hgroup><article  id="bayess-rule-up-to-proportion">

<ul>
<li><p>If data \(y\) are fixed, then \[
\begin{array}{rcl}
p(\theta\vert y ) &amp; = &amp; \frac{p(y\vert\theta)p(\theta)}{p(y)} \\
\\
&amp; \propto &amp; p(y\vert\theta)p(\theta) \\
\\
&amp; = &amp; p(y,\theta)
\end{array}
\]</p></li>
<li>Posterior proportional to likelihood times prior</li>
<li><p>Equivalently, posterior proportional to joint</p></li>
</ul>

</article></slide><slide class=''><hgroup><h2>Posterior Predictive Distribution</h2></hgroup><article  id="posterior-predictive-distribution">

<ul>
<li>Predict new data \(\tilde y\) based on observed data \(y\)</li>
<li>Marginalize out parameter from posterior \[
p(\tilde y \vert y) = \int_\Theta p(\tilde y \vert \theta) p(\theta \vert y)d\theta
\] averaging predcitions \(p(\tilde y\vert\theta)\) weighted by posterior \(p(\theta\vert y)\)</li>
<li>\(\Theta = \left\{\theta \vert p(\theta \vert y) &gt;0 \right\}\) is the support of \(p(\theta\vert y)\)</li>
<li><p>For discrete parameters \(\theta\), \[
p(\tilde y \vert y)=\sum_{\theta \in \Theta} p(\tilde y \vert \theta)p(\theta\vert y)
\]</p></li>
<li><p>Can mix continuous and discrete (integral as shorthand)</p></li>
</ul>

</article></slide><slide class=''><hgroup><h2>Event Probabilities</h2></hgroup><article  id="event-probabilities">

<ul>
<li>Recall that an event A is a collection of outcomes</li>
<li>Suppose event \(A\) is determined by indicator on parameters \[
f(\theta) = \begin{cases} 1 &amp; \mbox{if } \theta \in A \\ 
0 &amp; \mbox{if } \theta \notin A  \end{cases}
\]</li>
<li>e.g. \(f(\theta) = \theta_1 &gt; \theta_2\) for \(Pr(\theta_1 &gt; \theta_2\vert y)\)</li>
<li>Bayesian event probabilities compute posterior mass \[
P(A) = \int_\Theta f(\theta)p(\theta\vert y)d\theta
\]</li>
<li>Not frequentist, because involves parameter probabilities</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Laplace&#39;s Data and Problems</h2></hgroup><article  id="laplaces-data-and-problems">

<ul>
<li>Laplace&#39;s data on live birth in Paris from 1745-1770</li>
</ul>

<table class = 'rmdtable'>
<tr class="header">
<th align="left"><em>sex</em></th>
<th align="left"><em>live births</em></th>
</tr>
<tr class="odd">
<td align="left">female</td>
<td align="left">241,945</td>
</tr>
<tr class="even">
<td align="left">male</td>
<td align="left">251,527</td>
</tr>
</table>

<ul>
<li><p>Question 1 (Event Probability) Is a boy more likely to be born than a girl?</p></li>
<li><p>Question 2 (Estimate) What is the birth rate of boys vs. girls?</p></li>
<li><p>Bayes formulated the basic binomial model</p></li>
</ul>

<!-- Laplace solved the integral (Bayes couldn't) -->

</article></slide><slide class=''><hgroup><h2>Binomial Distribution</h2></hgroup><article  id="binomial-distribution">

<ul>
<li>Binomial distribution is number of successes \(y\) in \(N\) i.i.d. Bernoulli trials with chance of success \(\theta\)</li>
<li>If \(y_1,\ldots , y_N\sim\) Bernoulli(\(\theta\)), then \((y_1 + \ldots + y_N)\sim\) Binomial(\(N,\theta\))</li>
<li>The probabilistic model is \[
Binomial (y\vert N, \theta) = {{N}\choose{y}}\theta^y(1-\theta)^{N-y}
\] where the binomial coefficient normalizes for permutation (i.e., which subset of \(n\) has \(y_n =1\)): \[
{{N}\choose{y}} = \frac{N!}{y!(N-y)!}
\]</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Bayes&#39;s Binomial Model</h2></hgroup><article  id="bayess-binomial-model">

<ul>
<li>Data

<ul>
<li>\(y:\) total number of male live births (241,945)</li>
<li>\(N:\) total number of live births (493,472)</li>
</ul></li>
<li>Parameter

<ul>
<li>\(\theta \in (0,1)\): proportion of male live births</li>
</ul></li>
<li>Likelihood \[
p(y\vert N,\theta) = Binomial(y\vert N,\theta) ={{N}\choose{y}}\theta^y(1-\theta)^{N-y}
\]</li>
<li>Prior \[
p(\theta) = Uniform(\theta\vert 0,1)=1
\]</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Beta Distribution</h2></hgroup><article  id="beta-distribution">

<ul>
<li>Required for analytic posterior of Bayes&#39;s model</li>
<li>For parameters \(\alpha, \beta&gt;0\) and \(\theta\in (0,1)\), \[
Beta(\theta\vert \alpha, \beta) = \frac{1}{B(\alpha,\beta)}\theta^{\alpha-1}(1-\theta)^{\beta-1}
\]</li>
<li>Euler&#39;s Beta function is used to normalize \[
Beta(\theta\vert \alpha, \beta) = \int_0^1 u^{\alpha-1}(1-u)^{\beta-1}du = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}
\] where \(\Gamma()\) is continuous generalization of factorial</li>
<li>Note: \(Beta(\theta\vert 1,1)=Uniform(\theta, 0,1)\)</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Beta Distribution examples</h2></hgroup><article  id="beta-distribution-examples">

<p><img src="main_files/figure-html/unnamed-chunk-8-1.png" width="960" /></p>

</article></slide><slide class=''><hgroup><h2>Posterior distribution</h2></hgroup><article  id="posterior-distribution">

<ul>
<li>Given Baye&#39;s general formula for the posterior \[
p(\theta\vert y, N)=\frac{Binomial(y\vert N,\theta)Uniform(\theta\vert 0,1)}{\int_\Theta Binomial(y\vert N,\theta&#39;)p(\theta&#39;)d\theta&#39;}
\]</li>
<li>Using Euler&#39;s Beta function to normalize the posterior with final solution \[
p(\theta\vert y, N)= Beta(\theta\vert y+1, N-y+1)
\]</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Estimation</h2></hgroup><article  id="estimation">

<ul>
<li>Posterior is \(Beta(\theta \vert 1+ 241,945, 1+ 251,527)\)</li>
<li>Posterior mean: \[
\frac{1+ 241,945}{1+ 241,945+1+ 251,527}\approx 0.4902913
\]</li>
<li>Maximum likelihood estimate same as posterior mode (because of uniform prior) \[
\frac{241,945}{241,945+251,527}\approx 0.4902912
\]</li>
<li>As number of observations \(\to \infty\), MLE approaches to posterior mean</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Event Probability Inference</h2></hgroup><article  id="event-probability-inference">

<ul>
<li>What is probability that a male live birth is more likely than a female live birth?</li>
</ul>

<p>\[
\begin{array}{rcl}
Pr[\theta&gt;0.5] &amp; = &amp; \int_\Theta I[\theta&gt;0.5]p(\theta\vert y,N)d\theta \\
\\
&amp; = &amp; \int_{0.5}^1 p(\theta\vert y,N)d\theta \\
\\
&amp; = &amp; 1- F_{\theta\vert y,N}(0.5))\\
\\
&amp;\approx &amp; 1^{-42}
\end{array}
\] - \(I[\theta&gt;0.5] =1\) if \(\theta&gt;0.5\) is true and 0 otherwise - \(F_{\theta\vert y,N}\) is posterior cumulative distribution function (cdf)</p>

</article></slide><slide class=''><hgroup><h2>Bayesian Fisher Exact Test</h2></hgroup><article  id="bayesian-fisher-exact-test">

<ul>
<li>Suppose we observe the following data on handedness</li>
</ul>

<table class = 'rmdtable'>
<tr class="header">
<th align="left"><em>Drug</em></th>
<th align="left"><em>AEs</em></th>
<th align="left"><em>Successes</em></th>
<th align="left">Total</th>
</tr>
<tr class="odd">
<td align="left"><em>A</em></td>
<td align="left">9\((y_1)\)</td>
<td align="left">43</td>
<td align="left">52 \((N_1)\)</td>
</tr>
<tr class="even">
<td align="left"><em>B</em></td>
<td align="left">4\((y_2)\)</td>
<td align="left">44</td>
<td align="left">48\((N_2)\)</td>
</tr>
</table>

<ul>
<li>Assume likelihoods \(Binomial(y_k\vert N_k, \theta_k)\), uniform priors</li>
<li><p>Is Drug B better than A? \[
\begin{array}{rcl}
Pr[\theta_1 &gt; \theta_2 \vert y, N] &amp;=&amp;
\int_0^1 \int_0^1 p(\theta_1\vert y_1, N_1)p(\theta_2\vert y_2, N_2)d\theta_1d\theta_2\\
 &amp;\approx &amp; 0.91
 \end{array}
\]</p></li>
<li><p>Directly interpretable result; <em>not</em> frequentist procedure</p></li>
</ul>

</article></slide><slide class=''><hgroup><h2>Visualizing Posterior Difference</h2></hgroup><article  id="visualizing-posterior-difference">

<ul>
<li>Plot of posterior difference \(p(\theta_2 -\theta_1 \vert y N)\) (B-A)</li>
</ul>

<p><img src="main_files/figure-html/unnamed-chunk-9-1.png" width="720" /></p>

</article></slide><slide class=''><hgroup><h2>Visualizing Posterior Difference</h2></hgroup><article  id="visualizing-posterior-difference-1">

<ul>
<li>Plot of posterior difference \(p(\theta_2 -\theta_1 \vert y N)\) (B-A)</li>
</ul>

<p><img src="main_files/figure-html/unnamed-chunk-10-1.png" width="720" /></p>

</article></slide><slide class=''><hgroup><h2>Visualizing Posterior Difference</h2></hgroup><article  id="visualizing-posterior-difference-2" class="smaller">

<pre class = 'prettyprint lang-r'>library(R2OpenBUGS)
mymodel = function(){
  y1~ dbin(theta1, N1)
  y2~ dbin(theta2, N2)
  theta1~dbeta(1,1)
  theta2~dbeta(1,1)
  theta &lt;- theta2-theta1
}

data = list(N1 =52, N2 =48, y1=43, y2 = 44)
parameters=c(&quot;theta&quot;)
bugsInits &lt;- function() {
  return(list(theta1=0.5, theta2=0.5)) 
}

resbugs = bugs(data, inits=bugsInits, parameters.to.save=parameters, model.file=mymodel,
n.chains=1, n.iter=10000, n.burnin=2000, n.thin=1,
                debug=TRUE)</pre>

</article></slide><slide class=''><hgroup><h2>Visualizing Posterior Difference with STAN</h2></hgroup><article  id="visualizing-posterior-difference-with-stan" class="smaller">

<pre class = 'prettyprint lang-r'>modelString &lt;- &quot; 
  data{
    // First declare all variables in the data block
    int&lt;lower=0&gt; N1;
    int&lt;lower=0&gt; y1;
    int&lt;lower=0&gt; N2;
    int&lt;lower=0&gt; y2;
  }
  parameters{
    real&lt;lower=0, upper=1&gt; theta1;
    real&lt;lower=0, upper=1&gt; theta2;
  }
  model{
    theta1 ~ beta(1,1);
    y1 ~ binomial(N1, theta1);
    theta2 ~ beta(1,1);
    y2 ~ binomial(N2, theta2);
  }
  generated quantities{
    real theta;
  theta &lt;- theta1 - theta2;
  }
&quot; </pre>

</article></slide><slide class=''><hgroup><h2>Visualizing Posterior Difference with STAN: compling in C++</h2></hgroup><article  id="visualizing-posterior-difference-with-stan-compling-in-c">

<pre class = 'prettyprint lang-r'>library(rstan)
stanDso &lt;- stan_model(model_code = modelString)</pre>

</article></slide><slide class=''><hgroup><h2>Visualizing Posterior Difference with STAN: sampling from the posterior distribution</h2></hgroup><article  id="visualizing-posterior-difference-with-stan-sampling-from-the-posterior-distribution">

<pre class = 'prettyprint lang-r'>dataList = list(N1 =52, N2 =48, y1=43, y2 = 44)

stanFit &lt;- sampling(object = stanDso, 
                    data = dataList, 
                    chains = 1, iter = 9000, warmup = 1000, thin = 1)

samples &lt;- extract(stanFit)
thetas_post &lt;- samples[[&#39;theta&#39;]]</pre>

</article></slide><slide class=''><hgroup><h2>Visualizing Posterior Difference</h2></hgroup><article  id="visualizing-posterior-difference-3">

<ul>
<li>Plot of posterior difference \(p(\theta_2 -\theta_1 \vert y N)\) (B-A)</li>
<li>compute 95% credible interval</li>
</ul>

<pre class = 'prettyprint lang-r'>plot(density(thetas_post), main= &quot;posterior probability&quot;, 
     xlab=expression(theta[2]-theta[1]) )</pre>

<p><img src="main_files/figure-html/unnamed-chunk-14-1.png" width="288" style="display: block; margin: auto;" /></p>

</article></slide><slide class=''><hgroup><h2>Visualizing Posterior Difference</h2></hgroup><article  id="visualizing-posterior-difference-4">

<ul>
<li>Plot of posterior difference \(p(\theta_2 -\theta_1 \vert y N)\) (B-A)</li>
<li>compute 95% credible interval</li>
</ul>

<pre class = 'prettyprint lang-r'>quantile(thetas_post, probs=c(.025, .5, .975))</pre>

<pre >##        2.5%         50%       97.5% 
## -0.05240022  0.08454148  0.21821925</pre>

</article></slide><slide class=''><hgroup><h2>Conjugate Priors</h2></hgroup><article  id="conjugate-priors">

<ul>
<li>Family \(F\) is a conjugate prior for family \(G\) if

<ul>
<li>prior in \(F\) and</li>
<li>likelihood in \(G\)</li>
<li>entails posterior in \(F\)</li>
</ul></li>
<li>Before MCMC techniques became practical, Bayesian analysis mostly involved conjugate priors</li>
<li>Still widely used because analytic solutions are more efficient than MCMC</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Beta is Conjugate to Binomial</h2></hgroup><article  id="beta-is-conjugate-to-binomial">



<p>\[
\begin{array}{rcl}
p(\theta\vert y, N, \alpha,\beta) &amp;\approx&amp; p(\theta\vert\alpha,\beta)p(y\vert N,\theta)\\
\\
&amp;=&amp; Beta(\theta\vert\alpha,\beta)Binomial(y\vert N,\theta)\\
\\
&amp;=&amp; \frac{1}{B(\alpha, \beta)}\theta^{\alpha-1}(1-\theta)^{\beta-1}{{N}\choose{y}}\theta^y(1-\theta)^{N-y}\\
\\
&amp;\approx&amp;\theta^{y+\alpha-1}(1-\theta)^{N-y+\beta-1}\\
\\
&amp;\approx&amp;Beta(\theta\vert \alpha + y, \beta +(N-y))
\end{array}
\]</p>

</article></slide><slide class=''><hgroup><h2>Chaining Updates</h2></hgroup><article  id="chaining-updates">

<ul>
<li>Start with prior \(Beta(\theta\vert\alpha,\beta)\)</li>
<li>Receive binomial data in \(K\) stages \((y_1, N_1),\ldots , (y_K, N_K)\)</li>
<li>After \((y_1, N_1)\), posterior is \(Beta(\theta\vert \alpha + y_1, \beta +N_1-y_1)\)</li>
<li>Use as prior for \((y_2, N_2)\) with posterior \[
Beta(\theta\vert \alpha + y_1 +y_2, \beta +(N_1-y_1) + (N_2-y_2))
\]</li>
<li>Lather, rinse, repeate until final posterior \[
Beta(\theta\vert \alpha + y_1 +\ldots +y_K, \beta +(N_1 +\ldots N_K) - (y_1+\ldots +y_K))
\]</li>
<li>Same results as if we had updated with combined data \[
Beta(y_1+\ldots +y_K, N_1 +\ldots N_K)
\]</li>
</ul>

</article></slide><slide class=''><hgroup><h2>MAP Estimator</h2></hgroup><article  id="map-estimator">

<ul>
<li>For a Bayesian model \(p(y,\theta) = p(y\vert\theta)p(\theta)\), the max a posteriori (MAP) estimate maximizes the posterior \[
  \begin{array}{rcl}
\theta^* &amp; = &amp; arg max_\theta p(\theta\vert y)\\
\\
&amp; = &amp; arg max_\theta \frac{p(y\vert \theta)p(\theta)}{p(y)}\\
\\
&amp; = &amp; arg max_\theta p(y\vert \theta)p(\theta)\\
\\
&amp; = &amp; arg max_\theta \log p(y\vert \theta) + \log p(\theta)\\
\end{array}
\]</li>
<li><em>not</em> Bayesian because it does not integrate over uncertainty</li>
<li><em>not</em> frequentist because of distributions over parameters</li>
</ul>

</article></slide><slide class=''><hgroup><h2>MAP and the MLE</h2></hgroup><article  id="map-and-the-mle">

<ul>
<li>MAP estimate reduces to the MLE if the prior is uniform, i.e., \[
  p(\theta) = c
  \] because \[
\begin{array}{rcl}
  \theta^* &amp; = &amp; arg max_\theta p(y\vert \theta)p(\theta)\\
  \\
  &amp; = &amp; arg max_\theta p(y\vert \theta)c\\
  \\
  &amp; = &amp; arg max_\theta p(y\vert \theta)\\
  \end{array}
  \]</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Penalized Maximum Likelihood</h2></hgroup><article  id="penalized-maximum-likelihood">

<ul>
<li>The MAP can be made palatable to frequentists via philosophical sleight of hand</li>
<li>Treat the negative log prior \(-\log p(\theta)\) as a <em>penalty</em></li>
<li>e.g. a \(Normal(\theta\vert\mu,\sigma)\) prior becomes a penalty function \[
\lambda_{\theta,\mu,\sigma} = - \left(
  \log\sigma + \frac{1}{2}\left(\frac{\theta-\mu}{\sigma}\right)^2\right)
\]

<ul>
<li>Maximize sum of log likelihood and negative penalty</li>
</ul></li>
</ul>

<p>\[
  \begin{array}{rcl}
  \theta^* &amp; = &amp; arg max_\theta \log p(y\vert \theta, x) - \lambda_{\theta,\mu,\sigma}\\
    \\
  &amp; = &amp; arg max_\theta \log p(y\vert \theta, x) + \log p(\theta\vert\mu,\sigma)
  \end{array}
  \]</p>

</article></slide><slide class=''><hgroup><h2>Proper Bayesian Point Estimates</h2></hgroup><article  id="proper-bayesian-point-estimates">

<ul>
<li><p>Choose estimate to minimize some loss function</p></li>
<li><p>To minimize expected squared error (L2 loss), \(E[(\theta - \theta^{&#39;})^2]\), use the posterior mean</p></li>
</ul>

<p>\[
\hat\theta = arg min_\theta E[(\theta - \theta^{&#39;})^2] = \int_\Theta \theta\times p(\theta\vert y)d\theta
\]</p>

<ul>
<li><p>To minimize expected absolute error (L1 loss), \(E[|\theta - \theta^{&#39;}|]\), use posterior median</p></li>
<li><p>Other loss (utility) functions possible, study of which falls under decision theory</p></li>
<li><p>All share property of involving full Bayesian inference.</p></li>
</ul>

</article></slide><slide class=''><hgroup><h2>Homework: OPTIMISE Trial</h2></hgroup><article  id="homework-optimise-trial">

<ul>
<li><p>OPTIMSE (Optimisation of Cardiovascular Management to Improve Surgical Outcome) is a multi-centre randomised controlled trial which was carried out to try and determine if the optimisation of Cardiovascular Management intervention was of benefit. JAMA. 2014;311(21):2181-2190. <a href='doi:10.1001/jama.2014.5305' title=''>doi:10.1001/jama.2014.5305</a></p></li>
<li><p>Patients were randomised to a cardiac output-guided hemodynamic therapy for intravenous fluid and a drug to increase heart muscle contraction (the inotrope, dopexamine) during and 6 hours following surgery (intervention group) or to usual care (control group).</p></li>
<li><p>The primary outcome measure was the relative risk (RR) of a composite of 30-day moderate or major complications and mortality.</p></li>
</ul>

</article></slide><slide class=''><hgroup><h2>Homework: OPTIMISE Trial</h2></hgroup><article  id="homework-optimise-trial-1">

<ul>
<li><p><strong>Results</strong></p></li>
<li><p>Focusing on the primary outcome measure, there were 158/364 (43.3%) and 134/366 (36.6%) patients with complication/mortality in the control and intervention group respectively. Using the standard statistical approach, the relative risk (95% confidence interval) = 0.84 (0.70-1.01), p=0.07 and absolute risk difference = 6.8% (-0.3% to 13.9%), p=0.07. The authors reasonably concluded that:</p></li>
<li><p>In a randomized trial of high-risk patients undergoing major gastrointestinal surgery, use of a cardiac output-guided hemodynamic therapy algorithm compared with usual care did not reduce a composite outcome of complications and 30-day mortality.</p></li>
</ul>

</article></slide><slide class=''><hgroup><h2>Homework: OPTIMISE Trial</h2></hgroup><article  id="homework-optimise-trial-2">

<ul>
<li><p>Focus on the absolute difference risk groups and repeat the analysis in a Bayesian framework.</p></li>
<li><p>Assuming a non-informative prior, for example Beta(1,1), for complication/mortality rate in both groups, derive the posterior distribution for the absolute difference risk.</p></li>
<li>Using the posterior distribution of the intervention group complication/mortality rate and the posterior distribution of the usual care group, simulate patients&#39; outcome to determine the proportion of cases in which the intervention would result in no complication/death while the usual care would result in complication/death:

<ul>
<li>what is the probability of patients to get benefit from the intervention compared with usual care?</li>
</ul></li>
</ul>

</article></slide><slide class=''><hgroup><h2>Getting the slides</h2></hgroup><article  id="getting-the-slides">

<ul>
<li>The slides for this course were created with Rmarkdown: <a href='http://rmarkdown.rstudio.com/' title=''>http://rmarkdown.rstudio.com/</a>.</li>
<li>They are available from <a href='https://github.com/berkeley3/BDA-IZSTO' title=''>https://github.com/berkeley3/BDA-IZSTO</a>.</li>
<li><p>To re-compile the slides:</p>

<ul>
<li>Download the directory containing the lectures from Github</li>
<li>In R open the .Rmd file and set the working directory to the lecture directory</li>
<li>Click the <em>KnitHTML</em> button on Rstudio or run the following commands:</li>
</ul></li>
</ul>

<pre class = 'prettyprint lang-r'>library(rmarkdown) 
render(&quot;main.Rmd&quot;)</pre></article></slide>


  <slide class="backdrop"></slide>

</slides>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "main_files/mathjax-local/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!-- map slide visiblity events into shiny -->
<script>
  (function() {
    if (window.jQuery) {
       window.jQuery(document).on('slideleave', function(e) {
         window.jQuery(e.target).trigger('hidden');
      });
       window.jQuery(document).on('slideenter', function(e) {
         window.jQuery(e.target).trigger('shown');
      });
    }
  })();
</script>

</body>
</html>
